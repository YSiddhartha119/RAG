import os
from io import BytesIO
import time
from langchain.tools import tool
from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_google_genai import ChatGoogleGenerativeAI
from unstract.llmwhisperer import LLMWhispererClientV2
import hashlib


# Environment setup
from dotenv import load_dotenv
load_dotenv()


def get_file_hash(file_path: str) -> str:
    """Compute MD5 hash of the file content."""
    hasher = hashlib.md5()
    with open(file_path, "rb") as f:
        while chunk := f.read(8192):
            hasher.update(chunk)
    return hasher.hexdigest()


embeddings_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/static-retrieval-mrl-en-v1"
)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=200)
model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)



# LLM Whisperer client setup
llm_whisperer = LLMWhispererClientV2(
    base_url="https://llmwhisperer-api.us-central.unstract.com/api/v2",
    api_key=os.getenv("LLM_WHISPERER_API_KEY")
)

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    You are a highly knowledgeable financial assistant. Use only the information provided in the context below to answer the user's question. Do not use prior knowledge.
    Use all the context that is given to you the best you can and give the answer.
    If the context does not contain the information needed, respond with:
    "The answer is not available in the provided document."
    Use the following context to answer the question.
    ---
    Context:
    {context}
    ---

    Question: {question}

    Answer:
    """
)

def pdf_to_text(file_path: str) -> str:
    """Extract text from PDF using LLM Whisperer"""
    result = llm_whisperer.whisper(file_path=file_path)
    
    while True:
        status = llm_whisperer.whisper_status(
            whisper_hash=result['whisper_hash']
        )
        if status['status'] == 'processed':
            result = llm_whisperer.whisper_retrieve(
                whisper_hash=result['whisper_hash']
            )
            return result['extraction']['result_text']
        time.sleep(5)


rag_chain=None

vector_store=None

CACHE_DIR = "./faiss_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

def setup_rag_system(file_path: str):
    """Process PDF and create FAISS vector store"""
    global vector_store


    file_hash = get_file_hash(file_path)
    cache_path = os.path.join(CACHE_DIR, file_hash)

 # Try loading cached FAISS index
    if os.path.exists(cache_path):
        try:
            print(f"Loading cached FAISS index for file hash {file_hash}...")
            vector_store = FAISS.load_local(
                cache_path,
                embeddings_model,
                allow_dangerous_deserialization=True
            )
            print("Loaded cached vector store successfully.")
            return vector_store
        except Exception as e:
            print(f"Failed to load cache: {e}. Reprocessing...")
    
    # Extract text from PDF
    extracted_text = pdf_to_text(file_path)
    print("Text Extracted....")
    
    # Split text
    chunks = text_splitter.split_text(extracted_text)
    print("Chunks created....\n")

    metadatas = [{"source": os.path.basename(file_path), "file_hash": file_hash} for _ in chunks]

    
    # Create FAISS index
    vector_store = FAISS.from_texts(
        texts=chunks,
        embedding=embeddings_model
    )
    vector_store.save_local(cache_path)
    print(f"Saved FAISS index cache at {cache_path}")

    # return vector_store
    

@tool 
def rag_qa_tool(query: str) -> str:
    """"
    Query the processed document using the FAISS-based RAG system.

    Args:
        query (str): The user's question to be answered based on the processed document.

    Returns:
        str: The answer generated by the RAG system, or an error message if no document
             has been processed or if an error occurs during processing.

    Description:
        This function checks if a document has been processed and a vector store exists.
        It then invokes the RAG chain to retrieve relevant context and generate an answer
        to the user's query.
    """
    if not vector_store:
        return "No documents processed. Process a PDF first."
    # rag_chain = (
    #     RunnableParallel({
    #         "context": vector_store.as_retriever(
    #             search_kwargs={'k': 10},
    #             ),
    #         "question": RunnablePassthrough()
    #     })
    #     | prompt
    #     | model
    # )
    rag_chain = RetrievalQA.from_chain_type(
        llm=model,
        retriever=vector_store.as_retriever(search_kwargs={"k": 20}),
        chain_type="stuff",               
        return_source_documents=True
    )

    try:
         return rag_chain.invoke(query)['result']
    except Exception as e:
        return f"Error processing query: {str(e)}"
    
